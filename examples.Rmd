---
title: "Remembering R hurdles"
author: "Alex"
date: "Sunday, August 23, 2015"
output: html_document
---

## Using Knitr

To update existing Rpubs document, delete it and upload a new one to the same
stub. It's the only way I've found.

Knitr runs Rmd in the current directory, and Knitr FAQ claims it's a bad style
to use setwd(). So keep setwd instruction as a comment and run it in the console
before doing development or debug in console.  
Also see reference to home directory and using file.path to get proper slashes
for your system.

```{r, eval=FALSE}
# setwd(file.path(normalizePath("~"),"folder","subfolder"))
```

## Storing data

To prevent re-downloads every time when you develop in Rmd with console,
keep the original dataset in a separate variable and see if it's intact,
using digest().

```{r, eval=FALSE}
remote<-"https://something.com/folder/file.csv.zip"
localzip<-basename(remote)
if(!file.exists(localzip)) download.file(remote,localzip)
# No need to unzip - read.csv() does it automatically.
# Run digest() after loading for the first time to get the checksum.
if(!exists("loaded") || digest(loaded) != "b223723dffbf2ec857526bcc32d2a31d")
   loaded <- read.csv(localzip)
# Keep original data frame for console work caching and make a mutable copy
working<-loaded
````
readr::read_csv() and data.table::fread() are faster, and even more faster, leave column names as original (could be done with check.names=FALSE in read.csv), strings not converted to factors (could be done with stringsAsFactors = FALSE in read.csv). But they have problems with files, downloaded with download.file() on Windows, if mode="wb" wasn't used with it.

This is how to fix column names after reading with fread
```{r}
setnames(DT,colnames(DT),make.names(colnames(DT)))
```

If names shouldn't be corrected, use check.names=FALSE with read.csv()

When colnames have to be fixed, use make.names()

This is a function to drop file extention (from name string)

```{r, eval=FALSE}
dropExtension <- function(filename) {
        filename %>%
                basename %>%
                strsplit("\\.", fixed=FALSE) %>%
                "[["(1) %>%
                "["(-length(.)) %>%
                paste(collapse=".")
        }
```

## Exploration

Count NAs in each column of a dataframe df

```{r, eval=FALSE}
df %>% sapply(function(x){sum(is.na(x))})
```

Code to parse and plot month-count tables,
copied as copy-paste from MS SQL Studio (as it's remote desktop to a server
in other network, DB access from R isn't a simple thing to use; copy-paste
is simpler).

```{r, eval=FALSE}
# SQL code to get results to copy
SELECT dateadd(Month, datediff(MONTH, 0, datefield),0), COUNT(*)
FROM XXX
where XXX
group by dateadd(MONTH, datediff(MONTH, 0, datefield),0)
order by dateadd(MONTH, datediff(MONTH, 0, datefield),0)
```

```{r, eval=FALSE}
## dd<-"copy-paste-here"
library(data.table)
library(dplyr)
makeDT <- function(dd)
        dd %>%
        strsplit("\n") %>%
        "[["(1) %>%
        (function(x)x[2:length(x)]) %>%
        strsplit("[ \t]") %>%
        as.data.table %>%
        "["(c(1,3)) %>% 
        t %>% 
        as.data.table %>% 
        rename(month=V1,count=V2) %>%
        "["(,month := as.Date(month)) %>%
        "["(,count := as.numeric(count))
smoothDT <- function(DT)
        DT %>%
        ggplot(aes(x=month,y=count, group=1)) + geom_point() + geom_smooth()
smoothIt <- function(dd) dd %>% makeDT %>% smoothDT
# or plot with scale
ggplot(makeDT(dd), aes(x=month,y=count, group=1)) + geom_point() +
        geom_smooth() + scale_y_continuous(limits=c(0,4200))
```

## Performance and memory

Theoretically, apply() functions don't improve performance compared to for(),
they are just more readable. Practically, there's more to it, see
[separate review](http://htmlpreview.github.io/?https://github.com/gagin/R-tricks/blob/master/performance.html).

To check object size in memory, use "pryr" package.
```{r, eval=FALSE}
large.object<-rep(999,99999)
library(pryr); object_size(large.object)
```

Function to display progress. It does 1mln records in 1.4 seconds with step 1000
on my PC.

```{r, eval=FALSE}
source("https://raw.githubusercontent.com/gagin/R-tricks/master/progress.R")
tick<-progress()
for(i in 1:10000) tick()
```

## Vectorized vs not vectorized

Ifelse() is vectorized, while if() is not.

```{r, eval=FALSE}
# Example
# Function aggr() checks variable "type" in provided dataframe "df"
# to be already in acceptable values vector "events", and otherwise
# changes variable to fixed string "to" if "pattern" is found in it

aggr<-function(df,pattern,to)
        dplyr::mutate(df,
                      type=ifelse(! type %in% events,
                                  ifelse(grepl(pattern,type,ignore.case=TRUE),
                                         to,
                                         type),
                                  type))

```

Vectorized versions of min() and max() are called pmin() and pmax() - they
call it "parallel" instead of "vectorized".

Single sign logical operators are vectorized, while double sign - not.

```{r, eval=FALSE}
c(TRUE,FALSE) & c(TRUE,FALSE) # Returns TRUE FALSE
c(TRUE,FALSE) && c(TRUE,FALSE) # Returns TRUE
```

## Plotting with ggplot

Plotting functions pay attention to your data types, so for barchart you
need to have factors.  
Don't forget to print the chart object when using from function - otherwise
it's returned, not printed.
Use pluses at the end of the line to let the variable know you aren't finished

```{r, eval=FALSE}
p<-ggplot(data=cities,aes(x=year,y=TotalEmissions,fill=counties))+
        ggtitle("PM2.5 emissions from vehicles comparison")+
        p<-p+ylab("PM2.5 emissions, hundred tons")+
        p<-p+guides(fill=FALSE)+
        p<-p+geom_bar(position="dodge",stat="identity")
print(p)
```

Use gridExtra() library to combine plots.
```{r, eval=FALSE}  
library(gridExtra)
png("file.png",width=700)
grid.arrange(p1,p1,ncol=2)
dev.off()
```

## Plotting with lattice

This is how custom axis labels are made.
```{r, eval=FALSE}
with(patternw,
     xyplot(steps/5 ~ as.numeric(interval) | weekpart,
            layout=1:2,
            xlab="Hour of the day",
            ylab="Number of steps per minute",
            type="l",
            scales=list(
                    # Instead of numeric, draw interval as time again,
                    # but in a simpler way this time - as xyplot()
                    # has some issues with POSIXlt
                    x=list(
                            at=seq(0,2400,200),
                            labels=as.character(seq(0,24,2))
                            )
                    )
            )
     )
```

# Handling text

Function to convert first letters of words to uppercase
```{r, eval=FALSE}
# Modified function from toupper() help page
.simpleCapDecap <- function(x) {
    s <- strsplit(x, " ")[[1]]
    paste(toupper(substring(s, 1, 1)), tolower(substring(s, 2)),
          sep = "", collapse = " ")
}
```

Ways to print without line numbers and names - use cat, use row.names argument
for data.frame print function.

```{r}
cat(c("First line","Second line"),sep="\n")
m<-matrix(c(1,2,3,4),ncol=2)
m
data.frame(m)
print(data.frame(m), row.names=FALSE)
```

For data types conversion, here's function that adds zeroes to numeric records
like 222 to have 0222 ready for translation to time 02:22
```{r, eval=FALSE}
f<-function(i){
        i.length<-nchar(as.character(i))
        if(i.length<4){
                prefix<-paste(rep(0,4-i.length),collapse="")
                paste0(prefix,i)}
        else i
        }
f(222) # Produces 0222
```

## Dplyr pipes and tidyr

Dplyr pipes are convenient. If sort function doesn't work, try to ungroup().

```{r, eval=FALSE}
vehicles<-subset(SCC,grepl("Vehicle",SCC.Level.Two,ignore.case=TRUE))
places<-c("06037","24510")
names(places)<-c("Los Angeles","Baltimore")
cities<-NEI %>%
        subset(fips %in% places) %>%
        subset(SCC %in% vehicles$SCC) %>%
        select(Emissions,year,fips) %>%
        rename(counties=fips) %>%
        group_by(year,counties) %>%
        summarize(TotalEmissions=sum(Emissions)/100)
#also, not from real code
# ungroup %>% arrange(desc(TotalEmissions))
```

## Neuralnet preparation and call

Tidyr can be used to convert factors to columns with 0/1 values, to use
with neural networks. For neural networks types are also important,
automatic coersion doesn't work here.

```{r, eval=FALSE}
bins<-data %>% # piped functions follow
        
        # make it narrow, don't touch numeric variables and IDs
        gather(catnames,catvalues,-Dress_ID,-Rating,-Recommendation,-rnumber) %>%
        
        # make single column out of them
        unite(newfactor,catnames,catvalues,sep=".") %>%
        
        # add a new column - it's "1" for every record
        mutate( is = 1) %>%
        
        # create a column from each factor, and where there's no record, add "0"
        spread(newfactor, is, fill = 0)
# Now let's make it back numeric, except for ID
bins[]<-lapply(bins,as.numeric)
```

Prepare trainers
UPDATE - this actually can be done by caret::createDataPartition()

```{r, eval=FALSE}
nr<-dim(bins)[1] # number of observations
share<-0.8 # this is our 80% parameter
set.seed(seed)
trainset<-sample.int(nr,round(share*nr))
trainers<-bins[trainset,]
testers<bins[-trainset,]
```

Call neuralnet() and compute

```{r, eval=FALSE}
#make list for neuralnet() call
cat(paste0(names(bins),sep="+"))

# call
bins.nn<-function(df,rep=1,hidden=c(1),threshold=0.1) {
        set.seed(seed)
        nn.obj<-neuralnet(Target ~ List-from-prev-func,
                          data=df,
                          hidden=hidden,
                          lifesign="full",
                          lifesign.step=2000,
                          threshold=threshold,
                          rep=rep)
        return(nn.obj)}

n1<-bins.nn(trainers,rep=1,hidden=c(5),threshold=0.02)

nfeat<-dim(bins)[2] 
res<-neuralnet::compute(n1,testers[,3:nfeat]) # Your columns instead of 3
qualify(round(res$net.result),testers$Recommendation)

```

Results assessment function
UPDATE - this actually can be done by caret::confusionMatrix()

```{r, eval=FALSE}
qualify<-function(real,guess){
  check<-table(real,guess)
  good.ones<-check[1,1]+check[2,2]
  bad.ones<-check[1,2]+check[2,1]
  paste0(as.character(round(100*good.ones/(good.ones+bad.ones))),'%')
  }
```

How to make a formula of looong list variables - instead of parse eval text=TRUE, I can use this:


```{r}
xyform <- function (y_var, x_vars) {
        # y_var: a length-one character vector
        # x_vars: a character vector of object names
        as.formula(sprintf("%s ~ %s", y_var, paste(x_vars, collapse = " + ")))
}
```

## Hints I dont' understand yet

From ["Most useful R trick"](http://stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick)
discussion on SO.

# Dumping variables to files

dput() converts a variable to R code that will create it. File option will
write it this way to file. dget() does the reverse.  
Supposedly this may make understanding object's structure easier.
Also to helpful for showing your problem in coding discussions.
```{r}
str(list(list(1,c=2),list(d=3,a=4,list(5,6,7))))
dput(list(list(1,c=2),list(d=3,a=4,list(5,6,7))), control="all")
# Don't forget to use control="all" to preserve structure as much as possible
summary(list(list(1,c=2),list(d=3,a=4,list(5,6,7))))
```

# Parallelization

Revolution R has "foreach" library that allows to do parallel calculations
with %dopar%, but some background parallelism service is needed.
TODO: research

```{r, eval=FALSE}
#install.packages("foreach")
library(foreach)
library(data.table)
system.time(ppp<-t(t(t(t(matrix(1:100000000,nrow=10000)))))

system.time(
        foreach(zz=1:100) %do% {ppp<-t(t(t(t(matrix(1:100000000,nrow=10000)))))}
        )
THIS IS BROKEN AND INCOMPLETE
```
