---
title: "Remembering R hurdles"
author: "Alex"
date: "Sunday, August 23, 2015"
output: html_document
---

## Using Knitr

To update existing Rpubs document, delete it and upload a new one to the same
stub. It's the only way I've found.

Knitr runs Rmd in the current directory, and Knitr FAQ claims it's a bad style
to use setwd(). So keep setwd instruction as a comment and run it in the console
before doing development or debug in console.  
Also see reference to home directory and using file.path to get proper slashes
for your system.

```{r, eval=FALSE}
# setwd(file.path(normalizePath("~"),"folder","subfolder"))
```

## Storing data

To prevent re-downloads every time when you develop in Rmd with console,
keep the original dataset in a separate variable and see if it's intact,
using digest().

```{r, eval=FALSE}
remote<-"https://something.com/folder/file.csv.zip"
localzip<-basename(remote)
if(!file.exists(localzip)) download.file(remote,localzip)
# No need to unzip - read.csv() does it automatically.
# Run digest() after loading for the first time to get the checksum.
if(!exists("loaded") || digest(loaded) != "b223723dffbf2ec857526bcc32d2a31d")
   loaded <- read.csv(localzip)
# Keep original data frame for console work caching and make a mutable copy
working<-loaded
````
readr::read_csv() and data.table::fread() are faster, and even more faster, leave column names as original (could be done with check.names=FALSE in read.csv), strings not converted to factors (could be done with stringsAsFactors = FALSE in read.csv). But they have problems with files, downloaded with download.file() on Windows, if mode="wb" wasn't used with it.

Remember not to break names when read.csv() by check.names=FALSE

## Exploration

Count NAs in each column of a dataframe df

```{r, eval=FALSE}
df %>% sapply(function(x){sum(is.na(x))})
```

Code to parse and plot month-count tables,
copied as copy-paste from MS SQL Studio (as it's remote desktop to a server
in other network, DB access from R isn't a simple thing to use; copy-paste
is simpler).
```{r}
## dd<-"copy-paste-here"
library(data.table)
library(dplyr)
makeDT <- function(dd)
        dd %>%
        strsplit("\n") %>%
        "[["(1) %>%
        (function(x)x[2:length(x)]) %>%
        strsplit("[ \t]") %>%
        as.data.table %>%
        "["(c(1,3)) %>% 
        t %>% 
        as.data.table %>% 
        rename(month=V1,count=V2) %>%
        "["(,month := as.Date(month)) %>%
        "["(,count := as.numeric(count))
smoothDT <- function(DT)
        DT %>%
        ggplot(aes(x=month,y=count, group=1)) + geom_point() + geom_smooth()
smoothIt <- function(dd) dd %>% makeDT %>% smoothDT
# or plot with scale
ggplot(makeDT(dd), aes(x=month,y=count, group=1)) + geom_point() +
        geom_smooth() + scale_y_continuous(limits=c(0,4200))
```

## Performance and memory

Theoretically, apply() functions don't improve performance compared to for(),
they are just more readable. Practically, there's more to it, see
[separate review](http://htmlpreview.github.io/?https://github.com/gagin/R-tricks/blob/master/performance.html).

To check object size in memory, use "pryr" package.
```{r, eval=FALSE}
large.object<-rep(999,99999)
library(pryr); object_size(large.object)
```

Function to display progress. It does 1mln records in 1.4 seconds with step 1000
on my PC.

```{r, eval=FALSE}
source("https://raw.githubusercontent.com/gagin/R-tricks/master/progress.R")
tick<-progress()
for(i in 1:10000) tick()
```

## Vectorized vs not vectorized

Ifelse() is vectorized, while if() is not.

```{r, eval=FALSE}
# Example
# Function aggr() checks variable "type" in provided dataframe "df"
# to be already in acceptible values vector "events", and otherwise
# changes variable to fixed string "to" if "pattern" is found in it

aggr<-function(df,pattern,to)
        dplyr::mutate(df,
                      type=ifelse(! type %in% events,
                                  ifelse(grepl(pattern,type,ignore.case=TRUE),
                                         to,
                                         type),
                                  type))

```

Vectorized versions of min() and max() are called pmin() and pmax() - they
call it "parallel" instead of "vectorized".

Single sign logical operators are vectorized, while double sign - not.

```{r, eval=FALSE}
c(TRUE,FALSE) & c(TRUE,FALSE) # Returns TRUE FALSE
c(TRUE,FALSE) && c(TRUE,FALSE) # Returns TRUE
```

## Plotting with ggplot

Plotting functions pay attention to your data types, so for barchart you
need to have factors.  
Don't forget to print the chart object when using from function - otherwise
it's returned, not printed.
Use pluses at the end of the line to let the variable know you aren't finished

```{r, eval=FALSE}
p<-ggplot(data=cities,aes(x=year,y=TotalEmissions,fill=counties))+
        ggtitle("PM2.5 emissions from vehicles comparison")+
        p<-p+ylab("PM2.5 emissions, hundred tons")+
        p<-p+guides(fill=FALSE)+
        p<-p+geom_bar(position="dodge",stat="identity")
print(p)
```

Use gridExtra() library to combine plots.
```{r, eval=FALSE}  
library(gridExtra)
png("file.png",width=700)
grid.arrange(p1,p1,ncol=2)
dev.off()
```

## Plotting with lattice

This is how custom axis labels are made.
```{r, eval=FALSE}
with(patternw,
     xyplot(steps/5 ~ as.numeric(interval) | weekpart,
            layout=1:2,
            xlab="Hour of the day",
            ylab="Number of steps per minute",
            type="l",
            scales=list(
                    # Instead of numeric, draw interval as time again,
                    # but in a simpler way this time - as xyplot()
                    # has some issues with POSIXlt
                    x=list(
                            at=seq(0,2400,200),
                            labels=as.character(seq(0,24,2))
                            )
                    )
            )
     )
```

# Handling text

Function to convert first letters of words to uppercase
```{r, eval=FALSE}
# Modified function from toupper() help page
.simpleCapDecap <- function(x) {
    s <- strsplit(x, " ")[[1]]
    paste(toupper(substring(s, 1, 1)), tolower(substring(s, 2)),
          sep = "", collapse = " ")
}
```

Ways to print without line numbers and names - use cat, use row.names argument
for data.frame print function.

```{r}
cat(c("First line","Second line"),sep="\n")
m<-matrix(c(1,2,3,4),ncol=2)
m
data.frame(m)
print(data.frame(m), row.names=FALSE)
```

For data types conversion, here's aunction that adds zeroes to numeric records
like 222 to have 0222 ready for translation to time 02:22
```{r, eval=FALSE}
f<-function(i){
        i.length<-nchar(as.character(i))
        if(i.length<4){
                prefix<-paste(rep(0,4-i.length),collapse="")
                paste0(prefix,i)}
        else i
        }
f(222) # Produces 0222
```

## Dplyr pipes and tidyr

Dplyr pipes are convenient. If sort function doesn't work, try to ungroup().

```{r, eval=FALSE}
vehicles<-subset(SCC,grepl("Vehicle",SCC.Level.Two,ignore.case=TRUE))
places<-c("06037","24510")
names(places)<-c("Los Angeles","Baltimore")
cities<-NEI %>%
        subset(fips %in% places) %>%
        subset(SCC %in% vehicles$SCC) %>%
        select(Emissions,year,fips) %>%
        rename(counties=fips) %>%
        group_by(year,counties) %>%
        summarize(TotalEmissions=sum(Emissions)/100)
#also, not from real code
# ungroup %>% arrange(desc(TotalEmissions))
```

## Neuralnet preparation and call

Tidyr can be used to convert factors to columns with 0/1 values, to use
with neural networks. For neural networks types are also important,
automatic coersion doesn't work here.

```{r, eval=FALSE}
bins<-data %>% # piped functions follow
        
        # make it narrow, don't touch numeric variables and IDs
        gather(catnames,catvalues,-Dress_ID,-Rating,-Recommendation,-rnumber) %>%
        
        # make single column out of them
        unite(newfactor,catnames,catvalues,sep=".") %>%
        
        # add a new column - it's "1" for every record
        mutate( is = 1) %>%
        
        # create a column from each factor, and where there's no record, add "0"
        spread(newfactor, is, fill = 0)
# Now let's make it back numeric, except for ID
bins[]<-lapply(bins,as.numeric)
```

Prepare trainers

```{r, eval=FALSE}
nr<-dim(bins)[1] # number of observations
share<-0.8 # this is our 80% parameter
set.seed(seed)
trainset<-sample.int(nr,round(share*nr))
trainers<-bins[trainset,]
testers<bins[-trainset,]
```

Call neuralnet() and compute

```{r, eval=FALSE}
#make list for neuralnet() call
cat(paste0(names(bins),sep="+"))

# call
bins.nn<-function(df,rep=1,hidden=c(1),threshold=0.1) {
        set.seed(seed)
        nn.obj<-neuralnet(Target ~ List-from-prev-func,
                          data=df,
                          hidden=hidden,
                          lifesign="full",
                          lifesign.step=2000,
                          threshold=threshold,
                          rep=rep)
        return(nn.obj)}

n1<-bins.nn(trainers,rep=1,hidden=c(5),threshold=0.02)

nfeat<-dim(bins)[2] 
res<-neuralnet::compute(n1,testers[,3:nfeat]) # Your columns instead of 3
qualify(round(res$net.result),testers$Recommendation)

```

Results assessment function

```{r, eval=FALSE}
qualify<-function(real,guess){
  check<-table(real,guess)
  good.ones<-check[1,1]+check[2,2]
  bad.ones<-check[1,2]+check[2,1]
  paste0(as.character(round(100*good.ones/(good.ones+bad.ones))),'%')
  }
```

## Hints I dont' understand yet

From ["Most useful R trick"](http://stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick)
discussion on SO.

# Dumping variables to files

dput() converts a variable to R code that will create it. File option will
write it this way to file. dget() does the reverse.  
Supposedly this may make understanding object's structure easier.
Also to helpful for showing your problem in coding discussions.
```{r}
str(list(list(1,c=2),list(d=3,a=4,list(5,6,7))))
dput(list(list(1,c=2),list(d=3,a=4,list(5,6,7))))
summary(list(list(1,c=2),list(d=3,a=4,list(5,6,7))))
```

# Parallelization

Revolution R has "foreach" library that allows to do parallel calculations
with %dopar%, but some background parallelism service is needed.
TODO: research

```{r, eval=FALSE}
#install.packages("foreach")
library(foreach)
library(data.table)
system.time(ppp<-t(t(t(t(matrix(1:100000000,nrow=10000)))))

system.time(
        foreach(zz=1:100) %do% {ppp<-t(t(t(t(matrix(1:100000000,nrow=10000)))))}
        )
THIS IS BROKEN AND INCOMPLETE
```
